{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型改为bert-base-chinese，可训练中文数据\n",
    "model_class, tokenizer_class, pretrained_weights=(BertModel,BertTokenizer,'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2182,  2003,  2070,  3793,  2000,  4372, 16044]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(pretrained_weights,\n",
    "                                    output_hidden_states=True,\n",
    "                                    output_attentions=True)\n",
    "input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n",
    "all_hidden_states, all_attentions = model(input_ids)[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(pretrained_weights, torchscript=True)\n",
    "traced_model = torch.jit.trace(model, (input_ids,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试样例（windows环境） \n",
    "\n",
    "    set  GLUE_DIR=D:\\conda_venv\\pytorch-transformers\\examples\\tests_samples\n",
    "    set TASK_NAME=MRPC\n",
    "\n",
    "    python run_glue.py --model_type bert --model_name_or_path bert-base-uncased --task_name %TASK_NAME% --do_train  --do_eval  --do_lower_case --data_dir %GLUE_DIR%\\%TASK_NAME% --max_seq_length 128 --per_gpu_eval_batch_size=8  --per_gpu_train_batch_size=8  --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir %GLUE_DIR%\\output\\%TASK_NAME%\n",
    "    \n",
    "\n",
    "\n",
    "07/28/2019 11:03:00 - INFO - __main__ -   ***** Eval results  *****  \n",
    "07/28/2019 11:03:00 - INFO - __main__ -     acc = 0.5  \n",
    "07/28/2019 11:03:00 - INFO - __main__ -     acc_and_f1 = 0.25  \n",
    "07/28/2019 11:03:00 - INFO - __main__ -     f1 = 0.0  \n",
    "\n",
    "测试成功"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用的数据集\n",
    "https://github.com/SophonPlus/ChineseNlpCorpus 里的外卖平台收集的用户评价 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# waimai_10k 说明\n",
    "0. **下载地址：** [Github](https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/waimai_10k/waimai_10k.csv)\n",
    "1. **数据概览：** 某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条\n",
    "2. **推荐实验：** 情感/观点/评论 倾向性分析\n",
    "2. **数据来源：** 某外卖平台\n",
    "3. **原数据集：** [中文短文本情感分析语料 外卖评价](https://download.csdn.net/download/cstkl/10236683)，网上搜集，具体作者、来源不详\n",
    "4. **加工处理：**\n",
    "    1. 将原来 2 个文件整合到 1 个文件中\n",
    "    2. 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_all = pd.read_csv('waimai_10k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论数目（总体）：11987\n",
      "评论数目（正向）：4000\n",
      "评论数目（负向）：7987\n"
     ]
    }
   ],
   "source": [
    "print('评论数目（总体）：%d' % pd_all.shape[0])\n",
    "print('评论数目（正向）：%d' % pd_all[pd_all.label==1].shape[0])\n",
    "print('评论数目（负向）：%d' % pd_all[pd_all.label==0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字段说明\n",
    "\n",
    "| 字段 | 说明 |\n",
    "| ---- | ---- |\n",
    "| label | 1 表示正向评论，0 表示负向评论 |\n",
    "| review | 评论内容 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>1</td>\n",
       "      <td>非常满意，感谢送货员和商家！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>1</td>\n",
       "      <td>送餐很快，卷饼里面感觉再多点酱汁比较好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>1</td>\n",
       "      <td>包装不错，干净卫生，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>0</td>\n",
       "      <td>菜不是很好吃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10845</th>\n",
       "      <td>0</td>\n",
       "      <td>味道很一般，比老车记差远了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11789</th>\n",
       "      <td>0</td>\n",
       "      <td>等了一个半小时还不送。电话是空号。呵呵了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>1</td>\n",
       "      <td>恩还不错，就是订餐的时候系统匹配错误。打电话调节了一下，从新下单才订上</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5158</th>\n",
       "      <td>0</td>\n",
       "      <td>味道有点淡，不过服务很好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10380</th>\n",
       "      <td>0</td>\n",
       "      <td>菜都凉的，感觉没熟，量太少！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1</td>\n",
       "      <td>很快。。布丁奶茶贼好喝</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>1</td>\n",
       "      <td>卷饼还行，粥太难喝，皮蛋瘦肉粥太腥了，还得自己下楼取，勉强好评</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511</th>\n",
       "      <td>0</td>\n",
       "      <td>东西很棒，送餐服务很不好。迟到半个小时，也没表达什么歉意。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1</td>\n",
       "      <td>好吃,速度快！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>1</td>\n",
       "      <td>看到来送外卖的大叔疲倦的样子，真是各行各业都不容易，国家何时能让大家共同富裕起来，至少别等级...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>0</td>\n",
       "      <td>太油腻了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>0</td>\n",
       "      <td>服务态度很好，东西也好吃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>0</td>\n",
       "      <td>下次能送早点就好啦,快饿过劲啦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>1</td>\n",
       "      <td>很好,配送员也很好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>0</td>\n",
       "      <td>送餐没准！！！想送就送，不想送就不送</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4563</th>\n",
       "      <td>0</td>\n",
       "      <td>还不错吧，配送小哥态度很好，速度略慢...肘子卷饼味道还是可以的，但是总觉得不像肘子更像红烧...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review\n",
       "2529       1                                     非常满意，感谢送货员和商家！\n",
       "1844       1                                送餐很快，卷饼里面感觉再多点酱汁比较好\n",
       "2479       1                                         包装不错，干净卫生，\n",
       "4782       0                                             菜不是很好吃\n",
       "10845      0                                      味道很一般，比老车记差远了\n",
       "11789      0                               等了一个半小时还不送。电话是空号。呵呵了\n",
       "3359       1                恩还不错，就是订餐的时候系统匹配错误。打电话调节了一下，从新下单才订上\n",
       "5158       0                                      味道有点淡，不过服务很好。\n",
       "10380      0                                     菜都凉的，感觉没熟，量太少！\n",
       "117        1                                        很快。。布丁奶茶贼好喝\n",
       "1150       1                    卷饼还行，粥太难喝，皮蛋瘦肉粥太腥了，还得自己下楼取，勉强好评\n",
       "8511       0                      东西很棒，送餐服务很不好。迟到半个小时，也没表达什么歉意。\n",
       "1067       1                                            好吃,速度快！\n",
       "1529       1  看到来送外卖的大叔疲倦的样子，真是各行各业都不容易，国家何时能让大家共同富裕起来，至少别等级...\n",
       "4405       0                                              太油腻了。\n",
       "6085       0                                       服务态度很好，东西也好吃\n",
       "6692       0                                    下次能送早点就好啦,快饿过劲啦\n",
       "3327       1                                          很好,配送员也很好\n",
       "4228       0                                 送餐没准！！！想送就送，不想送就不送\n",
       "4563       0  还不错吧，配送小哥态度很好，速度略慢...肘子卷饼味道还是可以的，但是总觉得不像肘子更像红烧..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_all.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构造平衡语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_positive = pd_all[pd_all.label==1]\n",
    "pd_negative = pd_all[pd_all.label==0]\n",
    "\n",
    "def get_balance_corpus(corpus_size, corpus_pos, corpus_neg):\n",
    "    sample_size = corpus_size // 2\n",
    "    pd_corpus_balance = pd.concat([corpus_pos.sample(sample_size, replace=corpus_pos.shape[0]<sample_size), \\\n",
    "                                   corpus_neg.sample(sample_size, replace=corpus_neg.shape[0]<sample_size)])\n",
    "    \n",
    "    print('评论数目（总体）：%d' % pd_corpus_balance.shape[0])\n",
    "    print('评论数目（正向）：%d' % pd_corpus_balance[pd_corpus_balance.label==1].shape[0])\n",
    "    print('评论数目（负向）：%d' % pd_corpus_balance[pd_corpus_balance.label==0].shape[0])    \n",
    "    \n",
    "    return pd_corpus_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论数目（总体）：8000\n",
      "评论数目（正向）：4000\n",
      "评论数目（负向）：4000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7103</th>\n",
       "      <td>0</td>\n",
       "      <td>肉，豆腐，蔬菜都不新鲜、味道差极，关门少骗人吧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>1</td>\n",
       "      <td>很配送员很好，辛苦了，面也很好，很快</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>1</td>\n",
       "      <td>很好吃，以后快餐就它了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>1</td>\n",
       "      <td>味道还行，锡纸包着，还能加热。皮蛋瘦肉粥做的很好比嘉和一品好吃。包装很精致。还有一次性手套，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>1</td>\n",
       "      <td>1、尖椒土豆丝，说好的尖椒呢。。。全是土豆丝。,2、大号的卷饼，有点太大了。。。。。。,3、粥不错</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>1</td>\n",
       "      <td>湄洲的饭菜还是比较好的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11265</th>\n",
       "      <td>0</td>\n",
       "      <td>接电话的小妹态度很不好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7153</th>\n",
       "      <td>0</td>\n",
       "      <td>配料太多，味道一般</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>1</td>\n",
       "      <td>送货真的很快。。。。。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10399</th>\n",
       "      <td>0</td>\n",
       "      <td>冷面难吃死了，还不如方便面，太垃圾了，</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review\n",
       "7103       0                            肉，豆腐，蔬菜都不新鲜、味道差极，关门少骗人吧\n",
       "708        1                                 很配送员很好，辛苦了，面也很好，很快\n",
       "3133       1                                        很好吃，以后快餐就它了\n",
       "1077       1  味道还行，锡纸包着，还能加热。皮蛋瘦肉粥做的很好比嘉和一品好吃。包装很精致。还有一次性手套，...\n",
       "1930       1  1、尖椒土豆丝，说好的尖椒呢。。。全是土豆丝。,2、大号的卷饼，有点太大了。。。。。。,3、粥不错\n",
       "3662       1                                        湄洲的饭菜还是比较好的\n",
       "11265      0                                       接电话的小妹态度很不好。\n",
       "7153       0                                          配料太多，味道一般\n",
       "2598       1                                        送货真的很快。。。。。\n",
       "10399      0                                冷面难吃死了，还不如方便面，太垃圾了，"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waimai_10k_ba_4000 = get_balance_corpus(8000, pd_positive, pd_negative)\n",
    "\n",
    "waimai_10k_ba_4000.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评论长度大多集中在100以内，由于bert测试样例使用的是长度是128，则将句子最长长度设定为128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(waimai_10k_ba_4000['review'].apply(lambda x:len(x)).values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waimai_10k_ba_4000['review'].apply(lambda x:len(x)).quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(waimai_10k_ba_4000['review'].apply(lambda x:len(x))<=128)/len(waimai_10k_ba_4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 构造训练BERT所需要的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看HuggingFace的样本实例\n",
    "https://github.com/huggingface/pytorch-transformers/blob/master/examples/utils_glue.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这意味着我们需要将我们的数据转换成ID,text_a.text_b,label的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_bert = pd.DataFrame({\n",
    "    'id':range(len(waimai_10k_ba_4000)),\n",
    "    'text_a':waimai_10k_ba_4000['review'].str.strip(),\n",
    "    'text_b':'None',\n",
    "    'label': waimai_10k_ba_4000['label']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0</td>\n",
       "      <td>很好！份量很足，香干回锅肉里的萝卜干很好吃，每份饭还送了咖啡，谢谢！</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>1</td>\n",
       "      <td>速度很快，送了蘸酱，但是上次电话订餐没有送。孩子喜欢吃马盏糕和小笼包</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>2</td>\n",
       "      <td>辣！很辣！很过瘾的辣！</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>3</td>\n",
       "      <td>挺好吃,就是小贵,如果没立减的话</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>4</td>\n",
       "      <td>送餐小哥态度特别好！好多肉呀好吃好吃！可我以为是可以选米饭换卷饼的？还在订单里自信地备注了</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                         text_a text_b  label\n",
       "478    0             很好！份量很足，香干回锅肉里的萝卜干很好吃，每份饭还送了咖啡，谢谢！   None      1\n",
       "3753   1             速度很快，送了蘸酱，但是上次电话订餐没有送。孩子喜欢吃马盏糕和小笼包   None      1\n",
       "609    2                                    辣！很辣！很过瘾的辣！   None      1\n",
       "1935   3                               挺好吃,就是小贵,如果没立减的话   None      1\n",
       "902    4  送餐小哥态度特别好！好多肉呀好吃好吃！可我以为是可以选米饭换卷饼的？还在订单里自信地备注了   None      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看DataProcessor发现，读取数据是需要tsv格式，utf-8的文件，我们按80%，20%的比例分割训练集与验证集，再将文件保存为tsv格式，命名改为train.tsv与dev.tsv（或者在自己的DataProcessor改写）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_bert=train_df_bert.sample(frac=1.0).reset_index(drop=True)\n",
    "train_df=train_df_bert[:int(len(train_df_bert)*0.8)]\n",
    "dev_df = train_df_bert[int(len(train_df_bert)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7446</td>\n",
       "      <td>麻酱两块钱，给了一瓶盖子量！地瓜片是生的！超级大差评，说的微辣，一打开全是辣油</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1166</td>\n",
       "      <td>纯肉得，有种肉夹馍的味道</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1736</td>\n",
       "      <td>味道不错，速度很快。</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1859</td>\n",
       "      <td>骑士太给力了，送餐快，直接到门口</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5133</td>\n",
       "      <td>送餐太慢，送过来都凉了</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                   text_a text_b  label\n",
       "0  7446  麻酱两块钱，给了一瓶盖子量！地瓜片是生的！超级大差评，说的微辣，一打开全是辣油   None      0\n",
       "1  1166                             纯肉得，有种肉夹馍的味道   None      1\n",
       "2  1736                               味道不错，速度很快。   None      1\n",
       "3  1859                         骑士太给力了，送餐快，直接到门口   None      1\n",
       "4  5133                              送餐太慢，送过来都凉了   None      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train.tsv', sep='\\t', index=False, header=True)\n",
    "dev_df.to_csv('data/dev.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看官方Mrpc数据集的示例，复制代码并改写_create_examples函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = line[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaimaiProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the WaimaiProcessor data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1]\n",
    "            label = line[3]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们将数据读取并转换为了InputExample格式，但是BERT模型并不能直接使用这个格式，我们还需要做转换改为InputFeatures的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里面需要查看convert_examples_to_features，_truncate_seq_pair函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer, output_mode,\n",
    "                                 cls_token_at_end=False, pad_on_left=False,\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = tokens_a + [sep_token]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [sep_token]\n",
    "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens = tokens + [cls_token]\n",
    "            segment_ids = segment_ids + [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "        else:\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            label_id = label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            label_id = float(example.label)\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的说，就是把我们的评论数据进行分词，并加上标识符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型已改为bert-base-chinese，可训练中文数据\n",
    "model_class, tokenizer_class, pretrained_weights=(BertModel,BertTokenizer,'bert-base-chinese')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "A = tokenizer.tokenize('这家店不好吃')\n",
    "cls_token='[CLS]'\n",
    "sep_token='[SEP]'\n",
    "add_token_sentences = [cls_token] + A + [sep_token] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '这', '家', '店', '不', '好', '吃', '[SEP]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_token_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CLS] + A + [SEP] + B + [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The convention in BERT is:\n",
    "# (a) For sequence pairs:\n",
    "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "#  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "# (b) For single sequences:\n",
    "#  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "#  type_ids:   0   0   0   0  0     0   0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再将add_token_sentences转为数字格式，最后将input_ids补充padding就行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(add_token_sentences)\n",
    "pad_token=0\n",
    "padding_length=max_seq_length-len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_ids + ([pad_token] * padding_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 6822,\n",
       " 2158,\n",
       " 2422,\n",
       " 680,\n",
       " 1963,\n",
       " 1392,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转向训练部分  \n",
    "作者原来使用的是BERT官方的分类器，但经过B站up主espresso:https://www.bilibili.com/video/av601688911 的实践发现，原来的网络结构在中文训练集上效果并不好，可以看里面视频的改进，主要是进行mean与maxpooling再合并，下面列举出他的实现与官方实现，后续进行测试，看看哪一个效果较好  \n",
    "from pytorch_transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "#from models.bert_model import BertModel\n",
    "\"\"\"使用mean max pool的方式进行情感分析\"\"\"\n",
    "class Bert_Sentiment_Analysis(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Bert_Sentiment_Analysis, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.dense = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
    "        self.final_dense = nn.Linear(config.hidden_size, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def compute_loss(self, predictions, labels):\n",
    "        # 将预测和标记的维度展平, 防止出现维度不一致\n",
    "        predictions = predictions.view(-1)\n",
    "        labels = labels.float().view(-1)\n",
    "        epsilon = 1e-8\n",
    "        # 交叉熵\n",
    "        loss =\\\n",
    "            - labels * torch.log(predictions + epsilon) - \\\n",
    "            (torch.tensor(1.0) - labels) * torch.log(torch.tensor(1.0) - predictions + epsilon)\n",
    "        # 求均值, 并返回可以反传的loss\n",
    "        # loss为一个实数\n",
    "        loss = torch.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, text_input, positional_enc, labels=None):\n",
    "        encoded_layers, _ = self.bert(text_input, positional_enc,\n",
    "                                    output_all_encoded_layers=True)\n",
    "        sequence_output = encoded_layers[2]\n",
    "        # # sequence_output的维度是[batch_size, seq_len, embed_dim]\n",
    "        avg_pooled = sequence_output.mean(1)\n",
    "        max_pooled = torch.max(sequence_output, dim=1)\n",
    "        pooled = torch.cat((avg_pooled, max_pooled[0]), dim=1)\n",
    "        pooled = self.dense(pooled)\n",
    "\n",
    "\n",
    "\n",
    "        # 下面是[batch_size, hidden_dim * 2] 到 [batch_size, 1]的映射\n",
    "        # 我们在这里要解决的是二分类问题\n",
    "\n",
    "        predictions = self.final_dense(pooled)\n",
    "\n",
    "        # 用sigmoid函数做激活, 返回0-1之间的值\n",
    "        predictions = self.activation(predictions)\n",
    "        if labels is not None:\n",
    "            # 计算loss\n",
    "            loss = self.compute_loss(predictions, labels)\n",
    "            return predictions, loss\n",
    "        else:\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertPreTrainedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-fda1ede9f9b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBertPreTrainedModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     r\"\"\"\n\u001b[0;32m      3\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mLabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcomputing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mregression\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mIndices\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertPreTrainedModel' is not defined"
     ]
    }
   ],
   "source": [
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        >>> \n",
    "        >>> model = BertForSequenceClassification(config)\n",
    "        >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        >>> labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        >>> outputs = model(input_ids, labels=labels)\n",
    "        >>> loss, logits = outputs[:2]\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
    "                position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask, head_mask=head_mask)\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里先使用官方的实现，把run_glue.py需要的内容裁剪出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig,BertForSequenceClassification, BertTokenizer)\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "# from utils_glue import (compute_metrics, convert_examples_to_features,\n",
    "#                         output_modes, processors)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = tuple(BertConfig.pretrained_config_archive_map.keys())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-base-uncased',\n",
       " 'bert-large-uncased',\n",
       " 'bert-base-cased',\n",
       " 'bert-large-cased',\n",
       " 'bert-base-multilingual-uncased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-chinese',\n",
       " 'bert-base-german-cased',\n",
       " 'bert-large-uncased-whole-word-masking',\n",
       " 'bert-large-cased-whole-word-masking',\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       " 'bert-base-cased-finetuned-mrpc')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return {\"acc\": simple_accuracy(preds, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_modes = {\n",
    "    \"waimai\": \"classification\"\n",
    "}\n",
    "\n",
    "GLUE_TASKS_NUM_LABELS = {\n",
    "    \"waimai\": 2\n",
    "}\n",
    "processors = {\n",
    "    \"waimai\": WaimaiProcessor,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他代码放在run_waimai.py中，或者直接看run_glue.py  \n",
    "下面附上参数配置说明  \n",
    "refer:https://blog.csdn.net/whuty1304/article/details/89457014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main函数一开始就是一大段的参数设置，我们需要重点关注的主要包括以下几个参数：\n",
    "\n",
    "- data_dir： 输入数据的文件目录，里面应该包含train val test三个文件分别用于训练、验证、测试\n",
    "- bert_model： 所使用的bert预训练模型，这里我们一般用到的是bert-base-chinese\n",
    "- task_name：训练任务的名称，其实就是用来获取我们为每个任务自定义的Processor类\n",
    "- model_save_pth： 训练完的模型参数的保存地址\n",
    "- max_seq_length：字符串的最大长度，越长需要越多的计算量，一般设置64或128，这里是128\n",
    "- do_train/do_eval：是否训练或验证\n",
    "- learning_rate：学习率，论文推荐了几个，5e-5, 3e-5, 2e-5\n",
    "- no_cuda：是否使用GPU加速，如果设置为False，双重否定表肯定，表示使用GPU训练模型\n",
    "- local_rank：这个参数可以不改，默认设置为-1\n",
    "- per_gpu_eval_batch_size 这里用的是CPU，将batch改为16或8（虽然参数写的是gpu，但如果没有gpu，会用cpu运算）\n",
    "- per_gpu_train_batch_size  这里用的是CPU，将batch改为16或8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行参数\n",
    "--data_dir D:\\project\\bert_classify\\data  \n",
    "--model_type bert  \n",
    "--model_name_or_path bert-base-chinese\n",
    "--task_name waimai  \n",
    "--output_dir D:\\project\\bert_classify\\output  \n",
    "--do_train  \n",
    "--do_eval   \n",
    "--per_gpu_eval_batch_size=16\n",
    "--per_gpu_train_batch_size=16  \n",
    "--num_train_epochs  50  \n",
    "--do_lower_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "attachments": {
    "_auto_0": {
     "image/png": "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"
    },
    "_auto_1": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8cAAABBCAYAAAD8DtYCAAATjElEQVR4Ae2dy5EkuQ2GcybkgU6yRAaMLzrIifVDt3Vg/JAD8mEPuvWho1+lgBT/BhYiCZL5qKysLyM6+ADwA/gys6e43TP77eXl5fb19bXY1+12Wz4/P5ePj4/l/f19eXt7W15fX5cfP78vv/36y8IFAQhAAAIQgAAEIAABCEAAAhC4IoHvV2yKniAAAQhAAAIQgAAEIAABCEAAAiMEOByP0MIXAhCAAAQgAAEIQAACEIAABC5JgMPxJW8rTUEAAhCAAAQgAAEIQAACEIDACAEOxyO08IUABCAAAQhAAAIQgAAEIACBSxLgcHzJ20pTEIAABCAAAQhAAAIQgAAEIDBCgMPxCC18IQABCEAAAhCAAAQgAAEIQOCSBDgcX/K20hQEIAABCEAAAhCAAAQgAAEIjBDgcDxCC18IQAACEIAABCAAAQhAAAIQuCQBDseXvK00BQEIQAACEIAABCAAAQhAAAIjBDgcj9DCFwIQgAAEIAABCEAAAhCAAAQuSYDD8SVvK01BAAIQgAAEIAABCEAAAhCAwAgBDscjtPCFAAQgAAEIQAACEIAABCAAgUsS+NMlu7pAU//+5z+WP//1b793YuvZy3SIhx/PzxwB3h++f/D9k++fc989lv/+Oc7zw/PD8zNHgD9/+fP3Ht8/+cnx3PtKFAQgAAEIQAACEIAABCAAAQhciACH4wvdTFqBAAQgAAEIQAACEIAABCAAgTkC3b9WXfuxtv/V37kSzhll/dZ6E4tRe9SM63OSoCoIQAACEIAABCAAAQhAAALXJ9B9ONbv/ccD4VEHvOxAutWtUp6anu/Xz+Xv90pzv6cYRghAAAIQgAAEIAABCEAAAhC4L4HpX6u2Q55d8bC8VztH5qnligdb8xMH6zuzezbR19uYQwACEIAABCAAAQhAAAIQgMCxBKYPx8eWOZbNDp7+0DoWjTcEIAABCEAAAhCAAAQgAAEIPBuB7l+rFpjaoVP7/qeppZ/Ayk96mc+MXdpnHY1Bqa+z1ktdEIAABCAAAQhAAAIQgAAErk5g+HCsQ13pkGt7/uDn5wYyrkt70Wd0bZqq8Uw3z2pSLzaWej9TvdQCAQhAAAIQgAAEIAABCEDgmQhs/mvV/mCqA2EvUB0evb/Xs/249r5nn/va1asOymevnfogAAEIQAACEIAABCAAAQhcmcD04dgf9K4MaOvedCjeWhc9CEAAAhCAAAQgAAEIQAACEJgnMH04Vkp+8ikSjBCAAAQgAAEIQAACEIAABCDwqARWH45j4/6wPPpTUvtptI837biO+Upri5mJK2n5vVif5fA/Qc/s0d9rM4cABCAAAQhAAAIQgAAEIACB+xHo/ge5dNjUqJL94dD2/AGxZeuJl56NyusPmKU96c6O0vQ5fR+t/iwms6su+Xlt2RghAAEIQAACEIAABCAAAQhA4FgC3YfjkUNcy7dlU+sln9490yj5Sjsbe2Izn5o97sd1Vht2CEAAAhCAAAQgAAEIQAACENiHwGa/Vq2fuGrcp1xUIQABCEAAAhCAAAQgAAEIQAAC2xP49vLycvv6+lrs63a7LZ+fn8vHx8fy/v6+vL29La+vr8uPn9+Xf/39L9PZ7Sekaw7NxMOP5+d//2/smZfwCu9P7NueB37zIlJhDQEIQAACEIAABCCwhkD3r1WvSUIsBCAAAQj8j0D8Dz0c8nkyILA/Ad67/RmTAQIQgMAVCHA4vsJdpAcIQGAXAnv9hJoD8S63C9GLEdjy/fPvXDwoXwwb7UAAAhCAwAoCHI5XwCMUAhB4TAI9H7pbPi3bGiLxQ7v/QN+rK41SrGzSKvnIFscYK3vUkF/cl78fzTf6ZfGZ3euX5ll8Zi9pju5Zjti316jZVZt8o0Zmtzj5xFhpZmMtXvsxfk2eGKsccT/mZA0BCEAAAhCYJcDheJYccRCAwMMR0Ifr2cLXxrfymnb80F/a69WIsXFtOqW9ln6sL/p6PT+PfrXcPsbPFe/3/Fz2bPQxfq44v+fnsq8dTbN1teylevyenytH3PNrP5d/NvoYP1dc9nzIjxECEIAABCBwVgKb/WvVZ22QuiAAAQiIgH14X/MBfm286thjjIcVq9X2jrpG8kdfqzHuxfoze9ZnFp/ZM/0ee/b8ZPaeHDWftf2V4mu59trv4WN1ckEAAhCAAARmCXA4niVHHAQgcFkC8SBwRKP2wb/nstr2PAAcod/baw+P6LN3/THfUeuMWWY/qs4t8tg9vFI/WzBBAwIQgAAEjiHAr1Ufw5ksEIAABIYIbH1AsMOG1/Tz3sIsRtfM4WUmp/Ix/pFAxjKz/1Gtf9V6Blo2y+Dtytj7HPnYUoy3a17yU15GCEAAAhCAQIkAh+MSFfYgAIGnJWAfrO/5oTr7YL+mNotdo+9zj3Ia9Z99AH2NsxpnjsvuX2Zf01u8h3Ht2UdbXFsdthevkp/5SLsUE+3yjdqsIQABCEAAAhkBfq06I4QdAhCAwIEE7IO9fdUOAWtK0cFjRj8eOEY1Rv3X9HnlWOPYYpnZ17Ax7doVba0apRFjtM8IAQhAAAIQuBcBDsf3Ik9eCEAAAg0CPYeLRvj/mXQwlmFrfem2xnvkbNXzyLaMZWY/unfVY8+hvo6ugXwQgAAEIACBjACH44wQdghA4GkIxAPkkY1b7jNfW9WnQ9KZez1jbRn/zL53Tz357d77rxhja7NzQQACEIAABO5FgMPxvciTFwIQgMAEATtAxEOFycRD5+xBo6YfS436I/mjb0/9MSbmV321+rP4zF7TVd6zj2v7i/FZv/H+2Pro6x45j+6RfBCAAAQgsC0B/kGubXmiBgEInJiA/7CsuX3ot8vWmtdaUIz8bcxialpxv3T4GNX2GjHW25Q7+mi/NMb4Uqz3Kdm9rny9n/bMz+8rLrPLrzZm8S27t9X0s/3s+WnZS/ltT1dmNz/v42OlkY0+XnqKadnk4/uL8WZr1eRjNS/5+zpKdtXCCAEIQAACECgR4HBcosIeBCBwSQJrPyyvjRdUfbi3tdf0c/nGMfNp2Vs25Wn5tGw98fLRWNIr7cnfxnvbfS2j8zPUXquhth97bPnN2mKO0rqlHf1Lvv6di/6sIQABCEAAAiLA4VgkGCEAgacmUPpAvQeQo/LsUfsza9rhinu33xOwN9u99fcjgzIEIAABCBxJ4NvLy8vt6+trsa/b7bZ8fn4uHx8fy/v7+/L29ra8vr4uP35+X3779Zcj6yIXBCAAAQhAAAIQgAAEIAABCEDgMAL8g1yHoSYRBCAAAQhAAAIQgAAEIAABCJyVwN0Px3v/PaC99c96Y6kLAhCAAAQgAAEIQAACEIAABPoJTP2dYx0449/h0X5MH/1kN/9oixqjdmnbOKOvOBtj7kzbx2bxXsvPs/7lW+pNttoYteWnPjO7/LNROtKVv/a11hj9tL/1WMqv3LJpbbm116rD+8eYaGvpXMUmZs/Y+1H30Bjfk++98x/FeW2eGqfsHcnsa+t69vjafbkKl62en8hJupGT/14UfbxNcfJp2eRb8pEt1qd9jS17ZjONWu619a+JV6x6rNVo9lqP0ijFytajLx9GCFyRwPDh2L9wfi448YWLL5v8SmNJz+/5ueJLe7LFseQb9/zaz6Vle7Ur+sd1LU77Jf+4Z+s1V7w/USvaR/P5ev1cedbqS2dmVO5aXbbvbebv136u/H7Pz80e14phhMAMAXueZi/F6h2Y0ZHGTOyzxdRY2b7ugZ+Lj9/zc9kZ5wkYz6tf/pnx89G+a6z07Jb0Svninl/7uenFdW1P+6UatGdapau2L19fg5/32DN/0/A+fh5tPflqMdqXhh99Tj9XTLy/0cdrMYfAlQkM/Vp1fFHsRbI9XY/+YmX9WZ/WY+xT/ccx8on2mfVI/lH92FfkkelF/9j/Wv0s/5F2PffqKfZutcT+j6zvXrmsZzG5Vw1XzbuG7Rb3ZE3+kXuid2sk5hF84/cI4+l7zeyP0OOZazzq+b0Xg7M/P6X6jmbVegay+jJ71sva+Ew/s987f1YfdgicicDwT463Kj6+qFvpSmdvfeU542i922V/EOxx7a2/R829mvrAuhe73jrwgwAEjiWg72sx6+j3Av3ZU9OL+qwhYARqz8vo87eW5tmf36y+zL6WTy3+6PtUq2O2/7PUX+uLfQgcSWC3w7Fe0JFmspfT7F7Xz3vyZPo9Gnv6HFGfMdPVyjfKVpq94976vXWM+nl+o7Fn9m/15Z8T7+f3rbcem8W0/M7M6IjaPBvli5y1XxpjfClWPi2baZfspZwje8rtY5TH2zSXTf7a11p27dvaz+XXM0qrx7fmY7m30Knpr90XG9Pxdcb9uFZev+81/L6/B97H5tFPuhq9XXu1Os3ubfJfM8b8UT/aYw2ZvVVbzNXy3ctm9bfq8P1FP1v7eD/39dY0euJrmtLP7PJrjbX6FJPZvV9kZLZafNZ/SUu5NPb0X8svDY09WvJlhMDVCOx2OG6B6n3pSn76BmL6tW8WpbhSPb1+pdiePdNfc83WV+NitXjbHvpr+s1iazx9T5lGj930amyUq1ZLj37Np6apnLW4LfZjz75/zTUqX1yrTtuPl/RjTFzHuC3XpbpMX3VvmWtGK7KI60yz5B/3/NrPTTtbZ/kze9RXTsXpPpT85Csfxci39HzJJt9nHyMPvxY/8Y3rHv7y8bo21+X35at8M+tSjHLNjLG+qF+zK1dml98jj637ZX3pudE89hoZxXUWH/W2Xsd6RtdWj8XY5VmpzkxvpP+opRytMcbEtcXanl2l+lva2CBwJQK7HI5LL9wotJqG3/fzrfRHdby//8Zm+3HtfbP5mt5q2vGbneqL+3vk9jXN6sc6veYV5mfvz9dn93Dm8hoz8Wti7pl7Td1bxnoGNvfvorfN3t/RWn3O0diSv9eL/ZX8/V6tZ6/p/ePcs4y2M6x9H7Vet6jT56nN98y/RQ+9Gr6/Ukxm9zE1JiMaXm90nj2/sQ5bxxi/9nPVEjW0r9HH+LnZ41oxGjO7/FpjVl9mN235lOqRrVaDj/Hz6F+ylfZiXJY/qz/qsYbAVQnscjheC6v2ksd9e9HjXk/umZgeXfPp+eaTae1ZX5Ybex+BLe5zX6ZzedmzaZfevXNV99jVRKb3eMb2vL+xPz1HI3dN9Y3E9PpuwTvWp/UW2r19tPx8PZrLX/fHj7JpjDHa7x0Vrxw+Lu7ZOl6Kj/tHrGN9ltPXmNmzGr1W5ruXPfLVuqc28/V+4uH3WnX3xKse6WitHFrX7No/Ytyjf6s7cvK9bNn/aP2+DuYQeHQChx+OWy+2wczsGfAsPrNn+nvb96xvT+29uRytrz8Yjs575nw8P/venXvzPSK/PsSK5GjOGC+dM4yxttHe9u5hi3pijyM1Z/kzu+Vak3+k1ppvzB9rzuw13TPsZ7XHXo+uOasvs+9d7xF8WjnW9t/S3psd+hA4G4Gh/5WTvXz2Auna+mUq6fl8yjs7Zvpb9OfrLeVr1V7y93qtWG+zmJ64Uj6vU5vX9LfgV8t59v3Yu9U7y/fsvaq3R6jzkWrUM6T3y8atL6/Zej6931Y1zGhmMdHu1zY3pme5dH9VT6wvs5u/7086M2NNRzX0cqvpZDWV4pTbbPqa0cliZu2lmr1WZve+e8zFT9pWj7+PvUwVn41RP/OP9WX+R9uz+jJ7Vu/a+BJv2+u91ubvzYMfBK5A4NvLy8vt6+trsa/b7bZ8fn4uHx8fy/v7+/L29ra8vr4uP35+X3779Zff+9ULaS9b6Sq9xOZX25eGdLXW6PNEn2jza8VrjLHajzHyi/vmL5tibYx+8on7PqY0V1y0eZ2Sj7dbrHzivrfZvGSXT80mexaf2Vv6FrvHJS5eW3V4m/bMz/ZtXbN7Lflrz+to76xj7E9r33trHntXn2IgPdtv7SnuGUdjJDbq3+95hrKX/GWz0dulJR1vU4y3+bnZtZZv1Pf7pflIvHxjjdqXvrebzdby8Tb5HzEqv8/la5Hd73nflt1stTivUZt7bT/3/q0cipG/aon7ZpdNvjbKz2x+LluMMR+/pxhpepv2amOMNb8YH328PdpifGav1bX1vurwtVuO2n7MLz+/Ly1v057383bbz3yifTReub1O1PB1tGzS8j5et8fuY31exdrofaK+t8X4aJNmptHSyWKjXTkZIXB1AlOH41ko9nLv+bLtrT/bN3EQgAAEMgKl71+lvUznWe3PwOrKPZZ6K+096/NN3xCAAAQgcAyBQw/Hx7REFgg8HwH7ELnm2vM/WvXU9ej19/TY4xM53Pu+9NRsPrHuGLd3Hz7/3rlib0etrcer9iaG/j7a3tX7Vd+MEIAABCBwHgIcjs9zL6gEAhCAAAQgAAEIQAACEIAABO5EYOgf5LpTjaSFAAQgAAEIQAACEIAABCAAAQjsSuDuh+P4a1Rbd7u3/tb1ogcBCEAAAhCAwLEEnv2zwrP3f+zTtn027t/2TFF8XgJTv1atlzD+fSDtR5zRT3bzj7aoMWqXto0z+oqzMebOtH1sFu+1/DzrX76l3mSrjVFbfuozs8s/G6UjXflrX2uN0U/7W4+l/Motm9aWW3utOrx/jIm2ls5VbGL2jL333EPx8b5bspL+lpq+1keYG4Nn7r91j676fKgv3/vMM3CWZ0f9zPTgGYzOz9L/aN0j/vfsce/7es/eRu4BvhA4O4H/AC6wHT8wvSyfAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行结果：由于是在自己笔记本上跑的，用的CPU,跑一个epoch都要3个多小时。。。\n",
    "跑出来的结果感觉也不太理想，只有56.8%，后续如果有GPU环境的话，再试试更多的epochs，改用B站UP主设计的网络结构，或者收集更多的评论数据了\n",
    "![_auto_1](attachment:_auto_1)\n",
    "![_auto_0](attachment:_auto_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载已经训练好的模型，发现测试的结果也是60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(r'D:\\project\\bert_classify\\output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(r'D:\\project\\bert_classify\\output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_judge(sentence):\n",
    "    max_seq_length =128\n",
    "    A = tokenizer.tokenize(sentence)\n",
    "    cls_token='[CLS]'\n",
    "    sep_token='[SEP]'\n",
    "    add_token_sentences = [cls_token] + A + [sep_token] \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(add_token_sentences)\n",
    "    pad_token=0\n",
    "    padding_length=max_seq_length-len(input_ids)\n",
    "    input_ids = input_ids + ([pad_token] * padding_length)\n",
    "    a= torch.tensor([input_ids], dtype=torch.long)\n",
    "    outputs= model(input_ids=a)\n",
    "    print(sentence+':'+predict_judge(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_judge(outputs):\n",
    "    if int(torch.argmax(outputs[0])) ==1:\n",
    "        return '好评'\n",
    "    else:\n",
    "        return '差评'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "腊牛肉好吃极了！:好评\n",
      "百度小哥速度超快提前半小时到，非常感谢，这么冷的天真是不容易，再次感谢:差评\n",
      "不错滴，锡纸包住的创意很好，还配了手套，妈妈再也不用担心我吃个卷饼吃的哪哪都是了，送餐小哥的态度也很好，赞一个～:差评\n",
      "配送及时，态度很好，五星好评:好评\n",
      "今天下雪。天气寒冷。辛苦送餐员了:好评\n"
     ]
    }
   ],
   "source": [
    "good_sentence_list = ['腊牛肉好吃极了！','百度小哥速度超快提前半小时到，非常感谢，这么冷的天真是不容易，再次感谢','不错滴，锡纸包住的创意很好，还配了手套，妈妈再也不用担心我吃个卷饼吃的哪哪都是了，送餐小哥的态度也很好，赞一个～',\n",
    "                     '配送及时，态度很好，五星好评','今天下雪。天气寒冷。辛苦送餐员了']\n",
    "for sentence in good_sentence_list:\n",
    "    sentiment_judge(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "送的时间太晚了，一个小时才送来:差评\n",
      "送货超慢，已经都凉透了，饼硬的不行。:好评\n",
      "送餐现在越来越不靠谱:好评\n",
      "下单一个半小时才确认，送餐太慢了:差评\n",
      "盖饭洒了，面也洒了成一坨不能吃了，最失败的一次订餐:差评\n"
     ]
    }
   ],
   "source": [
    "bad_sentence_list = ['送的时间太晚了，一个小时才送来','送货超慢，已经都凉透了，饼硬的不行。','送餐现在越来越不靠谱','下单一个半小时才确认，送餐太慢了',\n",
    "                    '盖饭洒了，面也洒了成一坨不能吃了，最失败的一次订餐']\n",
    "for sentence in bad_sentence_list:\n",
    "    sentiment_judge(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后期计划：  \n",
    "改进分类器的结构  \n",
    "使用中文全词覆盖BERT（Chinese BERT with Whole Word Masking） 地址：https://github.com/ymcui/Chinese-BERT-wwm  \n",
    "训练更多的epoch  \n",
    "收集更多的外卖评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
